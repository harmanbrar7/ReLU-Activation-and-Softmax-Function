# ReLU-Activation-and-Softmax-Function
activation functions solving Multiclass Classification problems w/ tensorflow 

run the ipynb files with the py files
be aware of any tensorflow compile issues on your IDE, the code will run regardless

The ReLU's non-linear behavior provides the needed ability to turn functions off until they are needed.
The non-linear activation function is responsible for disabling the input prior to and sometimes after the transition points.

In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector  ùê≥ is generated by a linear function which is applied to a softmax function. The softmax function converts  ùê≥
into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs will correspond to larger output probabilities.

